{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\user\\appdata\\local\\programs\\python\\python310-32\\lib\\site-packages (5.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "%pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Day Weather Forecast:\n",
      "Monday: 25°C, Sunny\n",
      "Tuesday: 22°C, Cloudy\n",
      "Wednesday: 18°C, Rainy\n",
      "Thursday: 20°C, Partly Cloudy\n",
      "Friday: 30°C, Sunny\n",
      "\n",
      "Hottest day: Friday with 30°C\n",
      "Sunny days: Monday, Friday\n",
      "Average Temperature: 23.00°C\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "with open(\"weather.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"html.parser\")\n",
    "table_rows = soup.find(\"tbody\").find_all(\"tr\")\n",
    "weather_data = []\n",
    "\n",
    "for row in table_rows:\n",
    "    cols = row.find_all(\"td\")\n",
    "    day = cols[0].text.strip()\n",
    "    temperature = int(cols[1].text.strip().replace(\"°C\", \"\"))  # Convert to integer\n",
    "    condition = cols[2].text.strip()\n",
    "    weather_data.append((day, temperature, condition))\n",
    "print(\"5-Day Weather Forecast:\")\n",
    "for day, temp, cond in weather_data:\n",
    "    print(f\"{day}: {temp}°C, {cond}\")\n",
    "max_temp_day = max(weather_data, key=lambda x: x[1])\n",
    "print(f\"\\nHottest day: {max_temp_day[0]} with {max_temp_day[1]}°C\")\n",
    "sunny_days = [day for day, temp, cond in weather_data if cond == \"Sunny\"]\n",
    "print(f\"Sunny days: {', '.join(sunny_days)}\")\n",
    "average_temp = sum(temp for _, temp, _ in weather_data) / len(weather_data)\n",
    "print(f\"Average Temperature: {average_temp:.2f}°C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = \"https://realpython.github.io/fake-jobs\"\n",
    "DB_NAME = \"jobs.db\"\n",
    "TABLE_NAME = \"jobs\"\n",
    "def create_table():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            job_title TEXT,\n",
    "            company_name TEXT,\n",
    "            location TEXT,\n",
    "            description TEXT,\n",
    "            application_link TEXT,\n",
    "            UNIQUE(job_title, company_name, location)\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "def scrape_jobs():\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    jobs = []\n",
    "    \n",
    "    for job_element in soup.find_all(\"div\", class_=\"card-content\"):\n",
    "        job_title = job_element.find(\"h2\", class_=\"title\").text.strip()\n",
    "        company_name = job_element.find(\"h3\", class_=\"company\").text.strip()\n",
    "        location = job_element.find(\"p\", class_=\"location\").text.strip()\n",
    "        description = job_element.find(\"div\", class_=\"description\").text.strip()\n",
    "        application_link = job_element.find(\"a\", string=\"Apply\")[\"href\"]\n",
    "        jobs.append((job_title, company_name, location, description, application_link))\n",
    "    return jobs\n",
    "\n",
    "def store_jobs(jobs):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    for job in jobs:\n",
    "        job_title, company_name, location, description, application_link = job\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO {TABLE_NAME} (job_title, company_name, location, description, application_link)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "            ON CONFLICT(job_title, company_name, location) DO UPDATE SET\n",
    "                description = excluded.description,\n",
    "                application_link = excluded.application_link;\n",
    "        \"\"\", (job_title, company_name, location, description, application_link))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "def filter_jobs(location=None, company=None):\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    cursor = conn.cursor()\n",
    "    query = f\"SELECT * FROM {TABLE_NAME} WHERE 1=1\"\n",
    "    params = []\n",
    "    \n",
    "    if location:\n",
    "        query += \" AND location = ?\"\n",
    "        params.append(location)\n",
    "    if company:\n",
    "        query += \" AND company_name = ?\"\n",
    "        params.append(company)\n",
    "    \n",
    "    cursor.execute(query, params)\n",
    "    jobs = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return jobs\n",
    "def export_to_csv(filename, jobs):\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Job Title\", \"Company Name\", \"Location\", \"Description\", \"Application Link\"])\n",
    "        writer.writerows(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laptop data scraped and saved to laptops.json.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://www.demoblaze.com/\"\n",
    "API_URL = \"https://api.demoblaze.com/bycat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "PAYLOAD = {\"cat\": \"notebook\"}  \n",
    "def fetch_laptops():\n",
    "    response = requests.post(API_URL, json=PAYLOAD, headers=HEADERS)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"Items\", [])\n",
    "    return []\n",
    "def parse_laptops(laptops):\n",
    "    laptop_list = []\n",
    "    for laptop in laptops:\n",
    "        name = laptop.get(\"title\", \"Unknown\")\n",
    "        price = f\"${laptop.get('price', 'N/A')}\"\n",
    "        description = laptop.get(\"desc\", \"No description available.\")\n",
    "        laptop_list.append({\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"description\": description\n",
    "        })\n",
    "    return laptop_list\n",
    "def save_to_json(data, filename=\"laptops.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    laptops = fetch_laptops()\n",
    "    parsed_laptops = parse_laptops(laptops)\n",
    "    save_to_json(parsed_laptops)\n",
    "    print(\"Laptop data scraped and saved to laptops.json.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
